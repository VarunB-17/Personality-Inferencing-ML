{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "data = pd.read_json(\"training_90_balanced.json\")\n",
    "data_test = pd.read_json(\"test_10.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  478\n",
      "[TaggedDocument(words=['adus', 'hurt', 'weak', 'little', 'kid', 'punishment', 'effective', 'criminal', 'sociopath', 'pedophilic', 'desire', 'sociopath', 'care', 'thing', 'hurt', 'hard', 'wire', 'pedophile', 'thing', 'put', 'solitary', 'environment', 'introspection', 'occur', 'may', 'fix', 'think', 'countryside', 'perfect', 'place', 'rehabilitate', 'case', 'contact', 'one', 'live', 'slow', 'move', 'environment', 'may', 'help', 'build', 'brain', 'farm', 'fish', 'hunt', 'would', 'best', 'method', 'help', 'catch', 'mentally', 'human', 'solitary', 'confinement', 'fix', 'people', 'problem', 'would', 'say', 'best', 'thing', 'treat', 'people', 'kindness', 'remember', 'kid', 'crush', 'old', 'cartoon', 'character', 'kindhearted', 'understand', 'kid', 'develop', 'would', 'imagine', 'pedophile', 'could', 'fall', 'love', 'woman', 'really', 'humble', 'sweet', 'despite', 'appearance', 'child', 'humble', 'sweet', 'heart', 'one', 'ur', 'serious', 'right', 'kind', 'measure', 'joule', 'also', 'nuance', 'find', 'word', 'people', 'often', 'use', 'frame', 'nitpick', 'inessential', 'detail', 'something', 'positive', 'therefore', 'care', 'stupid', 'shit', 'something', 'positive', 'care', 'deficiency', 'argument', 'nuanced', 'say', 'take', 'actual', 'crux', 'say', 'actually', 'invert', 'degree', 'would', 'accomplish', 'good', 'try', 'mere', 'matter', 'nuance', 'human', 'form', 'limit', 'lol', 'interestingly', 'enough', 'find', 'heavy', 'hand', 'feeler', 'type', 'use', 'briggs', 'definition', 'completely', 'unaware', 'people', 'suffer', 'happy', 'enjoy', 'make', 'question', 'core', 'foundation', 'apathy', 'learn', 'desensitization', 'describe', 'relate', 'v', 'apathy', 'due', 'emotional', 'selfishness', 'wonder', 'one', 'bad', 'think', 'come', 'many', 'u', 'actually', 'reject', 'emotion', 'uimately', 'illogical', 'come', 'somewhere', 'end', 'reject', 'internally', 'see', 'hypocrisy', 'people', 'general', 'think', 'really', 'fascinate', 'part', 'u', 'perspective', 'world', 'view', 'amoral', 'truely', 'regard', 'practicality', 'world', 'get', 'simple', 'fact', 'know', 'explain', 'simply', 'situation', 'soccer', 'mom', 'tell', 'immoral', 'believe', 'insert', 'philosophy', 'age', 'drive', 'gas', 'guzziling', 'suv', 'never', 'give', 'money', 'help', 'homo', 'feel', 'good', 'moral', 'superiority', 'moral', 'view', 'world', 'want', 'amoral', 'person', 'ever', 'live', 'villain', 'definition', 'hero', 'day', 'also', 'keep', 'mind', 'context', 'argument', 'philosophy', 'say', 'observance', 'go', 'relatively', 'fast', 'move', 'space', 'relatively', 'slow', 'move', 'time', 'global', 'universal', 'time', 'reference', 'physically', 'possible', 'travel', 'speed', 'light', 'would', 'accurate', 'think', 'time', 'stop', 'instead', 'everything', 'happen', 'instantaneously', 'juxtaposition', 'unlikely', 'thing', 'im', 'really', 'sure', 'anima', 'animus', 'yet', 'cant', 'provide', 'input', 'emotional', 'attraction', 'usually', 'tbh', 'easily', 'crush', 'person', 'online', 'without', 'ever', 'see', 'face', 'connect', 'someone', 'way', 'even', 'little', 'bam', 'im', 'clingy', 'little', 'shit', 'prolly', 'care', 'friendship', 'way', 'person', 'lol', 'cuphead', 'dark', 'soul', 'platformer', 'intp', 'dark', 'soul', 'typology', 'water', 'wet', 'enough', 'r', 'socialanxiety', 'think', 'value', 'university', 'study', 'far', 'return', 'investment', 'go', 'probably', 'good', 'get', 'technical', 'degree', 'reputable', 'cc', 'personally', 'find', 'much', 'easy', 'rely', 'ability', 'parse', 'information', 'rather', 'defer', 'long', 'list', 'alliance', 'enemy', 'miss', 'quite', 'lot', 'throw', 'information', 'simply', 'dislike', 'origin', 'speaker', 'sure', 'word', 'act', 'probably', 'fallacy', 'iq', 'sign', 'success', 'business', 'true', 'mean', 'know', 'whether', 'marry', 'person', 'date', 'marriage', 'investment', 'know', 'marry', 'person', 'date', 'people', 'compatible', 'awhile', 'compatible', 'see', 'ne', 'se', 'rick', 'cause', 'rick', 'utilize', 'tool', 'disposal', 'term', 'abide', 'social', 'dynamic', 'term', 'either', 'seem', 'way', 'abstract', 'se', 'se', 'rick', 'estp', 'see', 'lot', 'le', 'argue', 'lot', 'le', 'zany', 'contraption', 'test', 'way', 'get', 'around', 'want', 'word', 'say', 'estp', 'really', 'stick', 'social', 'situation', 'se', 'lot', 'aggressive', 'political', 'look', 'obvious', 'angle', 'rick', 'sort', 'glaze', 'past', 'shit', 'always', 'look', 'get', 'around', 'one', 'improve', 'abstract', 'paradigm', 'good', 'enough', 'oppose', 'se', 'let', 'strike', 'deal', 'rick', 'seem', 'overt', 'trouble', 'si', 'ni', 'large', 'margin', 'well', 'ni', 'make', 'far', 'sense', 'ignore', 'function'], tags=['I', 0])]\n"
     ]
    }
   ],
   "source": [
    "# pip install gensim\n",
    "# Taggeddocument for training data\n",
    "taggedDocuments = []\n",
    "for index, data_point_post in data.iterrows():\n",
    "    taggedDocument =  TaggedDocument(words=data.at[index, 'posts'], tags=[data.at[index, 'type']], axis=1)\n",
    "    taggedDocuments.append(taggedDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taggeddocument for test data\n",
    "taggedDocuments_test = []\n",
    "for index, data_point_post in data_test.iterrows():\n",
    "    taggedDocument =  TaggedDocument(words=data_test.at[index, 'posts'], tags=[data_test.at[index, 'type']])\n",
    "    taggedDocuments_test.append(taggedDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# creating the model\n",
    "model = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(taggedDocuments)\n",
    "\n",
    "model.train(taggedDocuments, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, taggedDocuments):\n",
    "    \"\"\"\n",
    "    infering vectors from the model \n",
    "    \"\"\"\n",
    "    sents = taggedDocuments[0:45865]\n",
    "    tags, posts = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return tags, posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing the posts and types\n",
    "type_train, posts_train = vec_for_learning(model, taggedDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_test, posts_test = vec_for_learning(model, taggedDocuments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, n_jobs=4)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying logistic regression\n",
    "# C ? = hyperparam\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=4, C=1e5)\n",
    "logreg.fit(posts_train, type_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.7348920524182144\n",
      "Testing F1 score: 0.7514792425228755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           E       0.45      0.70      0.55      2458\n",
      "           I       0.89      0.74      0.81      8149\n",
      "\n",
      "    accuracy                           0.73     10607\n",
      "   macro avg       0.67      0.72      0.68     10607\n",
      "weighted avg       0.79      0.73      0.75     10607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "type_pred = logreg.predict(posts_test)\n",
    "print('Testing accuracy %s' % accuracy_score(type_test, type_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(type_test, type_pred, average='weighted')))\n",
    "print(classification_report(type_test, type_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5895c19eaeaaf2f51c408f07c95071cd65f0794f3cdacfedf343cff40747f1b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
