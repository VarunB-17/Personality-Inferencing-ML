{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cffc1753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'lorenzo']\n"
     ]
    }
   ],
   "source": [
    "# Importing pandas and sklearn to visualize the data and to split the data as a training/test split set\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df39b5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>know intj tool use interaction people excuse a...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rap music ehh opp yeah know valid well know fa...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>preferably p hd low except wew lad video p min...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drink like wish could drink red wine give head...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>space program ah bad deal meing freelance max ...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>loose stop overthinking everything ruin friend...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>often men one put chance get burn otherwise no...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>early definitely relate quiet self destruction...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>one full list would impossibly long quotable f...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>technique information power share unless absol...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  type\n",
       "0  know intj tool use interaction people excuse a...  INTJ\n",
       "1  rap music ehh opp yeah know valid well know fa...  INTJ\n",
       "2  preferably p hd low except wew lad video p min...  INTJ\n",
       "3  drink like wish could drink red wine give head...  INTJ\n",
       "4  space program ah bad deal meing freelance max ...  INTJ\n",
       "5  loose stop overthinking everything ruin friend...  INTJ\n",
       "6  often men one put chance get burn otherwise no...  INTJ\n",
       "7  early definitely relate quiet self destruction...  INTJ\n",
       "8  one full list would impossibly long quotable f...  INTJ\n",
       "9  technique information power share unless absol...  INTJ"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data as a pandas dataframe\n",
    "data = pd.read_csv(\"dataset.csv\") \n",
    "# Shows the first 10 instances of the dataset\n",
    "data[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1006017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTP    24961\n",
       "INTJ    22427\n",
       "INFJ    14963\n",
       "INFP    12134\n",
       "ENTP    11725\n",
       "ENFP     6167\n",
       "ISTP     3424\n",
       "ENTJ     2955\n",
       "ESTP     1986\n",
       "ENFJ     1534\n",
       "ISTJ     1243\n",
       "ISFP      875\n",
       "ISFJ      650\n",
       "ESTJ      482\n",
       "ESFP      360\n",
       "ESFJ      181\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Counting the class frequencies to search for the class proportions in case of class imbalance\n",
    "data['type'].value_counts().plot.bar(x='type', y='Frequency')\n",
    "\n",
    "# A bar plot of the class distribution\n",
    "data['type'].value_counts()\n",
    "# Observation: A right skewed distribution, which is not quite similar to the actual distribution of the population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "778010dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Preprocessing (removing stopwords)\u001B[39;00m\n\u001B[0;32m      2\u001B[0m stopwords \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(stopwords\u001B[38;5;241m.\u001B[39mwords(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m----> 4\u001B[0m tokenized_data \u001B[38;5;241m=\u001B[39m \u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mposts\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m processed_data \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m tokenized_data:\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Artificial Intelligence\\Year 2\\ML_Testing\\venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001B[0m, in \u001B[0;36mword_tokenize\u001B[1;34m(text, language, preserve_line)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mword_tokenize\u001B[39m(text, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m, preserve_line\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;124;03m    Return a tokenized copy of *text*,\u001B[39;00m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;124;03m    using NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;124;03m    :type preserve_line: bool\u001B[39;00m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 129\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m [text] \u001B[38;5;28;01mif\u001B[39;00m preserve_line \u001B[38;5;28;01melse\u001B[39;00m \u001B[43msent_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m    131\u001B[0m         token \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m _treebank_word_tokenizer\u001B[38;5;241m.\u001B[39mtokenize(sent)\n\u001B[0;32m    132\u001B[0m     ]\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Artificial Intelligence\\Year 2\\ML_Testing\\venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001B[0m, in \u001B[0;36msent_tokenize\u001B[1;34m(text, language)\u001B[0m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;124;03musing NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;124;03m:param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    106\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m load(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizers/punkt/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlanguage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 107\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Artificial Intelligence\\Year 2\\ML_Testing\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.tokenize\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1272\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize\u001B[39m(\u001B[38;5;28mself\u001B[39m, text, realign_boundaries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1273\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1274\u001B[0m \u001B[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001B[39;00m\n\u001B[0;32m   1275\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msentences_from_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrealign_boundaries\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Artificial Intelligence\\Year 2\\ML_Testing\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.sentences_from_text\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentences_from_text\u001B[39m(\u001B[38;5;28mself\u001B[39m, text, realign_boundaries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1327\u001B[0m \u001B[38;5;124;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[0;32m   1328\u001B[0m \u001B[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[0;32m   1329\u001B[0m \u001B[38;5;124;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[0;32m   1330\u001B[0m \u001B[38;5;124;03m    follows the period.\u001B[39;00m\n\u001B[0;32m   1331\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [text[s:e] \u001B[38;5;28;01mfor\u001B[39;00m s, e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Artificial Intelligence\\Year 2\\ML_Testing\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentences_from_text\u001B[39m(\u001B[38;5;28mself\u001B[39m, text, realign_boundaries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1327\u001B[0m \u001B[38;5;124;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[0;32m   1328\u001B[0m \u001B[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[0;32m   1329\u001B[0m \u001B[38;5;124;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[0;32m   1330\u001B[0m \u001B[38;5;124;03m    follows the period.\u001B[39;00m\n\u001B[0;32m   1331\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [text[s:e] \u001B[38;5;28;01mfor\u001B[39;00m s, e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Artificial Intelligence\\Year 2\\ML_Testing\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.span_tokenize\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m realign_boundaries:\n\u001B[0;32m   1321\u001B[0m     slices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_realign_boundaries(text, slices)\n\u001B[1;32m-> 1322\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m slices:\n\u001B[0;32m   1323\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m (sentence\u001B[38;5;241m.\u001B[39mstart, sentence\u001B[38;5;241m.\u001B[39mstop)\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Artificial Intelligence\\Year 2\\ML_Testing\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._realign_boundaries\u001B[1;34m(self, text, slices)\u001B[0m\n\u001B[0;32m   1408\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1409\u001B[0m \u001B[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001B[39;00m\n\u001B[0;32m   1410\u001B[0m \u001B[38;5;124;03mshould otherwise be included in the same sentence.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1418\u001B[0m \u001B[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001B[39;00m\n\u001B[0;32m   1419\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1420\u001B[0m realign \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1421\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence1, sentence2 \u001B[38;5;129;01min\u001B[39;00m _pair_iter(slices):\n\u001B[0;32m   1422\u001B[0m     sentence1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mslice\u001B[39m(sentence1\u001B[38;5;241m.\u001B[39mstart \u001B[38;5;241m+\u001B[39m realign, sentence1\u001B[38;5;241m.\u001B[39mstop)\n\u001B[0;32m   1423\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sentence2:\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Artificial Intelligence\\Year 2\\ML_Testing\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001B[0m, in \u001B[0;36m_pair_iter\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m    316\u001B[0m iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(iterator)\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 318\u001B[0m     prev \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    319\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Artificial Intelligence\\Year 2\\ML_Testing\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._slices_from_text\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m   1393\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_slices_from_text\u001B[39m(\u001B[38;5;28mself\u001B[39m, text):\n\u001B[0;32m   1394\u001B[0m     last_break \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1395\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m match, context \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_match_potential_end_contexts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1396\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_contains_sentbreak(context):\n\u001B[0;32m   1397\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mslice\u001B[39m(last_break, match\u001B[38;5;241m.\u001B[39mend())\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Artificial Intelligence\\Year 2\\ML_Testing\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m   1373\u001B[0m before_words \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m   1374\u001B[0m matches \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m-> 1375\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m match \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lang_vars\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperiod_context_re\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinditer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m)):\n\u001B[0;32m   1376\u001B[0m     \u001B[38;5;66;03m# Ignore matches that have already been captured by matches to the right of this match\u001B[39;00m\n\u001B[0;32m   1377\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m matches \u001B[38;5;129;01mand\u001B[39;00m match\u001B[38;5;241m.\u001B[39mend() \u001B[38;5;241m>\u001B[39m before_start:\n\u001B[0;32m   1378\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Preprocessing (removing stopwords)\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "tokenized_data = []\n",
    "for sentence in data[\"posts\"]:\n",
    "    word_tokenize(sentence)\n",
    "\n",
    "\n",
    "print(data[\"\"])\n",
    "\n",
    "\n",
    "processed_data = []\n",
    " \n",
    "for w in tokenized_data:\n",
    "    if w not in stopwords:\n",
    "        processed_data.append(w)\n",
    "\n",
    "print(processed_data[0:10])\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a578b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model_selection model from the sklearn library to split t\n",
    "# Using the parameters x and y to .....\n",
    "train_test_split(some parameters)\n",
    "cross_validate(some parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4074d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}